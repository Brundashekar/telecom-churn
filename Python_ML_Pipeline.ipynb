{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python ML Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this magic command will capture runtime of every cell in this notebook\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display current working directory\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path of the input dataset (user input required)\n",
    "# input_path = input(\"Enter the path of the input dataset - \")\n",
    "\n",
    "input_path = r\"..\\new_data\\sample_10k.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column name of the target variable in the input data\n",
    "\n",
    "target = 'dep_var';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path of the output folder location (user input required)\n",
    "# output_path = input(\"Enter the path of the output folder - \")\n",
    "\n",
    "output_path = r\"..\\notebooks\\outputs7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the required libraries\n",
    "import numpy as np\n",
    "import pandas as pdra\n",
    "from pandas import Series\n",
    "import pandas.core.algorithms as algos\n",
    "import scipy.stats.stats as stats\n",
    "import os, sys, re, glob, gc, time, klib, traceback, string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from collections import Counter\n",
    "from mlxtend.evaluate import bias_variance_decomp\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, FunctionTransformer, PowerTransformer, MinMaxScaler, StandardScaler, KBinsDiscretizer\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE, VarianceThreshold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, matthews_corrcoef, log_loss\n",
    "from boruta import BorutaPy\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "import xgboost\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "# set up to ignore warnings\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "%matplotlib inline\n",
    "pd.options.display.float_format = '{:.2f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display function to display rows & columns with truncation\n",
    "def display_all(df):\n",
    "    with pd.option_context(\"display.max_rows\", 10000, \"display.max_columns\", 10000): \n",
    "        display(df)\n",
    "\n",
    "# function to check whether the path exists, if not creates a folder\n",
    "def path_exists(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_datepart converts datetime64 columns from df to many columns containing the information from the date. This applies changes inplace\n",
    "def add_datepart(df, fldnames, drop=True, time=False, errors=\"raise\"):\n",
    "    if isinstance(fldnames,str): \n",
    "        fldnames = [fldnames]\n",
    "    for fldname in fldnames:\n",
    "        fld = df[fldname]\n",
    "        fld_dtype = fld.dtype\n",
    "        if isinstance(fld_dtype, pd.core.dtypes.dtypes.DatetimeTZDtype):\n",
    "            fld_dtype = np.datetime64\n",
    "\n",
    "        if not np.issubdtype(fld_dtype, np.datetime64):\n",
    "            df[fldname] = fld = pd.to_datetime(fld, infer_datetime_format=True, errors=errors)\n",
    "        targ_pre = re.sub('[Dd]ate$', '', fldname)\n",
    "        attr = ['Year', 'Quarter', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear',\n",
    "                'Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start']\n",
    "        if time: attr = attr + ['Hour', 'Minute', 'Second']\n",
    "        for n in attr: df[targ_pre + n] = getattr(fld.dt, n.lower())\n",
    "#         df[targ_pre + 'Elapsed'] = fld.astype(np.int64) // 10 ** 9\n",
    "        if drop: df.drop(fldname, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the input dataset into df\n",
    "df = pd.read_csv(input_path, low_memory=False, parse_dates=[\"PURCHASE_DATE\", \"DISPOSED_DATE\"])\n",
    "print(df.shape)\n",
    "display_all(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename target variable as perf_flag\n",
    "df.rename(columns={target: 'perf_flag'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# replace fields that's entirely space (or empty) with NaN\n",
    "df.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify column names - change them to lowercase, strip spaces, replace space between words with underscore\n",
    "df.columns = df.columns.str.strip().str.lower().str.replace(' ','_')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the response rate of target variable\n",
    "ax = sns.countplot(df['perf_flag'], label=\"Count\")       \n",
    "NR, R = df['perf_flag'].value_counts(normalize=True)*100\n",
    "print(\"Non-Responders: {0:.2f}%\".format(NR))\n",
    "print(\"Responders: {0:.2f}%\".format(R))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# saving the columns and their datatypes for review (user to review this to identify any discrepancies with column datatypes)\n",
    "df.dtypes.to_csv(f'{dtype_path}\\\\Column_Datatypes_Original.csv')\n",
    "\n",
    "display_all(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing dtype of certain columns from numeric to string type (user input required)\n",
    "\n",
    "new_col_dtype = {'svc_vin_loyality_indx': 'str', 'sls_vin_loyalty_indx': 'str', 'svc_hhh_loyality_indx': 'str', 'family_composition': 'str', 'advg_home_owner': 'str', 'liquid_resources': 'str', 'target_net_worth_3_cd': 'str'}\n",
    "df = df.astype(new_col_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# information about the dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separating features and target\n",
    "y = df['perf_flag'].copy() # target\n",
    "X = df.drop('perf_flag', axis=1) # features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# viewing dtypes of the columns\n",
    "display_all(X.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage of missing values in each column of the input data, sorted in descending order\n",
    "miss_perc_by_col = (X.isnull().sum()/len(X))*100\n",
    "display_all(miss_perc_by_col[miss_perc_by_col > 0].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop variables with >80% missing values\n",
    "miss_perc_df = pd.DataFrame(miss_perc_by_col).reset_index()\n",
    "miss_perc_df.columns = ['col_nm', 'miss_perc']\n",
    "\n",
    "rm_miss_lst = miss_perc_df[miss_perc_df['miss_perc'] > 80]\n",
    "rm_miss_lst = list(rm_miss_lst['col_nm'])\n",
    "print('list of features with missing value >80% :', rm_miss_lst)\n",
    "\n",
    "X.drop(rm_miss_lst, axis=1, inplace=True)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of datetime variables\n",
    "s = (X.dtypes == 'datetime64[ns]')\n",
    "dt_time_cols = list(s[s].index)\n",
    "\n",
    "print(\"Date time variables:\")\n",
    "display_all(dt_time_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace datetime64 column with multiple columns containing date related information\n",
    "add_datepart(X, dt_time_cols)\n",
    "print(X.shape)\n",
    "display_all(X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of numeric variables\n",
    "s = (X.dtypes != 'object') & (X.dtypes != 'datetime64[ns]') & (X.dtypes != 'category')\n",
    "numeric_cols = list(s[s].index)\n",
    "\n",
    "print(\"Number of numeric varibles:\", len(numeric_cols))\n",
    "# display_all(numeric_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# information about data\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# descriptive statistics of the input dataset\n",
    "desc_stats = X.describe(percentiles=[0.25, .5, .75, .9, .95, .99, .995])\n",
    "display_all(desc_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating eda summary report - data deep dive\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "profile = ProfileReport(X, title='Pandas Profiling Report', html={'style':{'full_width':True}}, minimal=True)\n",
    "profile.to_file(f\"{eda_path}\\\\01_EDA_Summary_Report.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numeric data distribution and kde(kernel density estimation) by target vraiable\n",
    "pp = PdfPages(f'{eda_path}\\\\03_Numeric_Data_Distribution.pdf')\n",
    "\n",
    "for i in iter(X.select_dtypes(include = ['float64', 'float32', 'int64', 'int32', 'int16', 'int8']).columns):\n",
    "    tmp = X[i]\n",
    "    sns.set_style(\"darkgrid\")\n",
    "    fig = plt.figure()\n",
    "    fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, sharex=True)\n",
    "    \n",
    "    sns.histplot(data=tmp, x=tmp, bins=20, kde=True, ax=ax1).set_title('Histogram and Kernel density estimation')\n",
    "    sns.kdeplot(data=tmp, x=tmp, hue=y, ax=ax2)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    pp.savefig(fig)\n",
    "    plt.close()\n",
    "\n",
    "pp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to visualize categorical data distributions\n",
    "\n",
    "# Imports\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib.colors import LinearSegmentedColormap, to_rgb\n",
    "from typing import Any, Dict, Optional, Tuple, Union\n",
    "\n",
    "from klib.utils import (\n",
    "    _corr_selector,\n",
    "    _missing_vals,\n",
    "    _validate_input_bool,\n",
    "    _validate_input_int,\n",
    "    _validate_input_range,\n",
    "    _validate_input_smaller,\n",
    "    _validate_input_sum_larger,\n",
    ")\n",
    "\n",
    "# Functions\n",
    "\n",
    "# Categorical Plot\n",
    "def custom_cat_plot(\n",
    "    data: pd.DataFrame,\n",
    "    figsize: Tuple = (18, 18),\n",
    "    top: int = 3,\n",
    "    bottom: int = 3,\n",
    "    bar_color_top: str = \"#5ab4ac\",\n",
    "    bar_color_bottom: str = \"#d8b365\",\n",
    "):\n",
    "    \"\"\" Two-dimensional visualization of the number and frequency of categorical features.\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pd.DataFrame\n",
    "        2D dataset that can be coerced into Pandas DataFrame. If a Pandas DataFrame \\\n",
    "        is provided, the index/column information is used to label the plots\n",
    "    figsize : Tuple, optional\n",
    "        Use to control the figure size, by default (18, 18)\n",
    "    top : int, optional\n",
    "        Show the \"top\" most frequent values in a column, by default 3\n",
    "    bottom : int, optional\n",
    "        Show the \"bottom\" most frequent values in a column, by default 3\n",
    "    bar_color_top : str, optional\n",
    "        Use to control the color of the bars indicating the most common values, by \\\n",
    "        default \"#5ab4ac\"\n",
    "    bar_color_bottom : str, optional\n",
    "        Use to control the color of the bars indicating the least common values, by \\\n",
    "        default \"#d8b365\"\n",
    "    cmap : str, optional\n",
    "        The mapping from data values to color space, by default \"BrBG\"\n",
    "    Returns\n",
    "    -------\n",
    "    Gridspec\n",
    "        gs: Figure with array of Axes objects\n",
    "    \"\"\"\n",
    "\n",
    "    # Validate Inputs\n",
    "    _validate_input_int(top, \"top\")\n",
    "    _validate_input_int(bottom, \"bottom\")\n",
    "    _validate_input_range(top, \"top\", 0, data.shape[1])\n",
    "    _validate_input_range(bottom, \"bottom\", 0, data.shape[1])\n",
    "    _validate_input_sum_larger(1, \"top and bottom\", top, bottom)\n",
    "\n",
    "    data = pd.DataFrame(data).copy()\n",
    "    cols = data.select_dtypes(exclude=[\"number\"]).columns.tolist()\n",
    "    data = data[cols]\n",
    "\n",
    "    if len(cols) == 0:\n",
    "        print(\"No columns with categorical data were detected.\")\n",
    "        return None\n",
    "\n",
    "    for col in data.columns:\n",
    "        if data[col].dtype.name in (\"category\", \"string\"):\n",
    "            data[col] = data[col].astype(\"object\")\n",
    "\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    gs = fig.add_gridspec(nrows=6, ncols=len(cols), wspace=0.21)\n",
    "\n",
    "    for count, col in enumerate(cols):\n",
    "        n_unique = data[col].nunique(dropna=True)\n",
    "        value_counts = data[col].value_counts()\n",
    "        lim_top, lim_bot = top, bottom\n",
    "\n",
    "        if n_unique < top + bottom:\n",
    "            lim_top = int(n_unique // 2)\n",
    "            lim_bot = int(n_unique // 2) + 1\n",
    "\n",
    "        if n_unique <= 2:\n",
    "            lim_top = lim_bot = int(n_unique // 2)\n",
    "\n",
    "        value_counts_top = value_counts[0:lim_top]\n",
    "        value_counts_idx_top = value_counts_top.index.tolist()\n",
    "        value_counts_bot = value_counts[-lim_bot:]\n",
    "        value_counts_idx_bot = value_counts_bot.index.tolist()\n",
    "\n",
    "        if top == 0:\n",
    "            value_counts_top = value_counts_idx_top = []\n",
    "\n",
    "        if bottom == 0:\n",
    "            value_counts_bot = value_counts_idx_bot = []\n",
    "\n",
    "        data.loc[data[col].isin(value_counts_idx_top), col] = 10\n",
    "        data.loc[data[col].isin(value_counts_idx_bot), col] = 0\n",
    "        data.loc[((data[col] != 10) & (data[col] != 0)), col] = 5\n",
    "        data[col] = data[col].rolling(2, min_periods=1).mean()\n",
    "\n",
    "        value_counts_idx_top = [elem[:20] for elem in value_counts_idx_top]\n",
    "        value_counts_idx_bot = [elem[:20] for elem in value_counts_idx_bot]\n",
    "        sum_top = sum(value_counts_top)\n",
    "        sum_bot = sum(value_counts_bot)\n",
    "\n",
    "        # Barcharts\n",
    "        \n",
    "        ax_top = fig.add_subplot(gs[:1, count : count + 1])\n",
    "        ax_top.set_title(col)\n",
    "        ax_top.bar(\n",
    "            value_counts_idx_top, value_counts_top, color=bar_color_top, width=0.85\n",
    "        )\n",
    "        ax_top.bar(\n",
    "            value_counts_idx_bot, value_counts_bot, color=bar_color_bottom, width=0.85\n",
    "        )\n",
    "        ax_top.set(frame_on=False)\n",
    "        ax_top.tick_params(axis=\"x\", labelrotation=90)\n",
    "\n",
    "        # Summary stats\n",
    "        ax_bottom = fig.add_subplot(gs[1:2, count : count + 1])\n",
    "        plt.subplots_adjust(hspace=0.075)\n",
    "        ax_bottom.get_yaxis().set_visible(False)\n",
    "        ax_bottom.get_xaxis().set_visible(False)\n",
    "        ax_bottom.set(frame_on=False)\n",
    "        ax_bottom.text(\n",
    "            0,\n",
    "            0,\n",
    "            f\"Unique values: {n_unique}\\n\\n\"\n",
    "            f\"Top {lim_top} vals: {sum_top} ({sum_top/data.shape[0]*100:.1f}%)\\n\"\n",
    "            f\"Bot {lim_bot} vals: {sum_bot} ({sum_bot/data.shape[0]*100:.1f}%)\",\n",
    "            transform=ax_bottom.transAxes,\n",
    "            color=\"#111111\",\n",
    "            fontsize=11,\n",
    "        )\n",
    "\n",
    "    gs.figure.suptitle(\n",
    "        \"Categorical data plot\", x=0.5, y=0.91, fontsize=18, color=\"#111111\"\n",
    "    )\n",
    "\n",
    "    return gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical data distribution at overall level and by perf_flag type\n",
    "plot = custom_cat_plot(X.select_dtypes(include=['category', 'object']), (36, 20), top=5, bottom=5)\n",
    "plot.figure.savefig(f\"{eda_path}\\\\04_Categorical_Data_Distribution_01.pdf\", bbox_inches='tight', dpi=100)\n",
    "plt.close()\n",
    "\n",
    "# distribution by target variable\n",
    "def plot_fig(plot, pdf):\n",
    "    try:\n",
    "        fig = plot.draw()\n",
    "    except:\n",
    "        fig = plot\n",
    "    pdf.savefig(fig.fig, height=10, width=18, dpi=500, bbox_inches='tight', pad_inches=0.5)\n",
    "    plt.close()\n",
    "\n",
    "pp = PdfPages(f'{eda_path}\\\\04_Categorical_Data_Distribution_02.pdf')\n",
    "for i in iter(X.select_dtypes(include = ['category', 'object']).columns):    \n",
    "    sns.set_style(\"darkgrid\")\n",
    "    g = sns.catplot(x=i, col=\"perf_flag\", data=df, kind=\"count\")\n",
    "    if i.endswith('indx'):\n",
    "        for ax in g.axes.ravel():\n",
    "            ax.set_xticklabels(ax.get_xticklabels(), rotation=70)\n",
    "    plot_fig(g, pp)\n",
    "\n",
    "pp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating correlation martrix\n",
    "corr = np.round(X.corr().abs(),2)\n",
    "corr = corr.style.background_gradient(cmap='Oranges')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation matrix, abs correlations above threshold (>0.7)\n",
    "\n",
    "corr_mat = klib.corr_mat(X.select_dtypes(include = ['float64', 'float32', 'int64', 'int32', 'int16', 'int8']), split='high', threshold=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing the eda outputs into a excel\n",
    "with pd.ExcelWriter(f'{eda_path}\\\\02_Data_Exploration.xlsx') as writer:\n",
    "    pd.DataFrame(X.dtypes).to_excel(writer, sheet_name='InputData_Column_Datatypes')\n",
    "    pd.DataFrame(miss_perc_df).to_excel(writer, sheet_name='Missing_Percentage')\n",
    "    pd.DataFrame(desc_stats).to_excel(writer, sheet_name='Numeric_Descriptive_Stats')\n",
    "    corr.to_excel(writer, sheet_name='Correlation_Matrix')\n",
    "    corr_mat.to_excel(writer, sheet_name='Highly_Correlated_Features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training and validation data\n",
    "# The split is based on a random number generator. Supplying a numeric value to the random_state argument guarantees we get the same split every time we run this script\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "X_valid.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "y_valid.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print('X_train:', X_train.shape, ' X_valid:', X_valid.shape, ' y_train:', y_train.shape, ' y_valid:', y_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing value treatment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing missing values - Numeric columns\n",
    "\n",
    "# Get names of columns with missing values\n",
    "cols_with_missing = [col for col in X_train.select_dtypes(exclude=['category', 'object']).columns\n",
    "                     if X_train.select_dtypes(exclude=['category', 'object'])[col].isnull().any()]\n",
    "\n",
    "# Make copy to avoid changing original data (when imputing)\n",
    "X_train_num = X_train.select_dtypes(exclude=['category', 'object'])\n",
    "X_valid_num = X_valid.select_dtypes(exclude=['category', 'object'])\n",
    "\n",
    "# Make new columns indicating what will be imputed\n",
    "for col in cols_with_missing:\n",
    "    X_train_num[col + '_was_missing'] = X_train_num[col].isnull()\n",
    "    X_valid_num[col + '_was_missing'] = X_valid_num[col].isnull()\n",
    "\n",
    "# Mean Imputation (other available strategies - median, most frequent, constant)\n",
    "imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imputed_X_train_num = pd.DataFrame(imp_mean.fit_transform(X_train_num))\n",
    "imputed_X_valid_num = pd.DataFrame(imp_mean.transform(X_valid_num))\n",
    "\n",
    "# Imputation removed column names; put them back\n",
    "imputed_X_train_num.columns = X_train_num.columns\n",
    "imputed_X_valid_num.columns = X_valid_num.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing missing values - Categorical columns\n",
    "\n",
    "# Make copy to avoid changing original data (when imputing)\n",
    "X_train_cat = X_train.select_dtypes(include=['category', 'object'])\n",
    "X_valid_cat = X_valid.select_dtypes(include=['category', 'object'])\n",
    "\n",
    "# Imputation (other available strategy - most frequent)\n",
    "imp_cat = SimpleImputer(strategy='constant', fill_value=\"NA\")\n",
    "imputed_X_train_cat = pd.DataFrame(imp_cat.fit_transform(X_train_cat))\n",
    "imputed_X_valid_cat = pd.DataFrame(imp_cat.transform(X_valid_cat))\n",
    "\n",
    "# Imputation removed column names; put them back\n",
    "imputed_X_train_cat.columns = X_train_cat.columns\n",
    "imputed_X_valid_cat.columns = X_valid_cat.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating imputed numeric & categorical dataframes \n",
    "imputed_X_train = pd.concat([imputed_X_train_num, imputed_X_train_cat], axis=1)\n",
    "imputed_X_valid = pd.concat([imputed_X_valid_num, imputed_X_valid_cat], axis=1)\n",
    "\n",
    "print('imputed_X_train:', imputed_X_train.shape, ' imputed_X_valid:', imputed_X_valid.shape)\n",
    "print('Sample rows from imputed_X_train:')\n",
    "display_all(imputed_X_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding for categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of categorical variables\n",
    "s = ((imputed_X_train.dtypes == 'object') | (imputed_X_train.dtypes == 'category'))\n",
    "object_cols = list(s[s].index)\n",
    "\n",
    "print(\"Number of Categorical variables: \", len(object_cols))\n",
    "print(object_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply one-hot encoder to each column with categorical data\n",
    "OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(imputed_X_train[object_cols]))\n",
    "OH_cols_valid = pd.DataFrame(OH_encoder.transform(imputed_X_valid[object_cols]))\n",
    "\n",
    "OH_cols_train.columns = OH_encoder.get_feature_names(object_cols)\n",
    "OH_cols_valid.columns = OH_encoder.get_feature_names(object_cols)\n",
    "\n",
    "# One-hot encoding removed index; put it back\n",
    "OH_cols_train.index = imputed_X_train.index\n",
    "OH_cols_valid.index = imputed_X_valid.index\n",
    "\n",
    "# Remove categorical columns (will replace with one-hot encoding)\n",
    "num_X_train = imputed_X_train.drop(object_cols, axis=1)\n",
    "num_X_valid = imputed_X_valid.drop(object_cols, axis=1)\n",
    "\n",
    "# Add one-hot encoded columns to numerical features\n",
    "imputed_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\n",
    "imputed_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print sample rows after one-hot encoding\n",
    "print('imputed_X_train:', imputed_X_train.shape, ' imputed_X_valid:', imputed_X_valid.shape)\n",
    "print('Sample rows from imputed_X_train:')\n",
    "display_all(imputed_X_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_X_train = imputed_X_train.astype(np.float32)\n",
    "imputed_X_valid = imputed_X_valid.astype(np.float32)\n",
    "# imputed_X_train = klib.convert_datatypes(imputed_X_train)\n",
    "# imputed_X_valid = klib.convert_datatypes(imputed_X_valid)\n",
    "\n",
    "print(\"imputed_X_train\", imputed_X_train.info())\n",
    "print(\"-\"*60)\n",
    "print(\"imputed_X_valid\", imputed_X_valid.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection (Prior to Variable transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection with boruta (before variable transformation)\n",
    "clf = RandomForestClassifier(n_estimators=50, n_jobs=-1, max_depth=5, random_state=0)\n",
    "\n",
    "boruta_feature_selector = BorutaPy(clf, n_estimators=100, random_state=42, verbose=2, max_iter=50, perc=75)\n",
    "boruta_feature_selector.fit(imputed_X_train.values, y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating list of selected features (using boruta)\n",
    "features = imputed_X_train.columns\n",
    "\n",
    "final_features = list()\n",
    "indexes = np.where(boruta_feature_selector.support_ == True)\n",
    "for x in np.nditer(indexes):\n",
    "    final_features.append(features[x])\n",
    "print('number of shortlisted features: ', len(final_features))\n",
    "pd.DataFrame(final_features).to_csv(f\"{fs_path}\\\\02_FeaturesList_Before_VarTransform_Boruta.csv\", index=False, header=['column_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset the input dataframe with selected features only\n",
    "imputed_X_train = pd.DataFrame(boruta_feature_selector.transform(imputed_X_train.values), columns = final_features)\n",
    "imputed_X_valid = pd.DataFrame(boruta_feature_selector.transform(imputed_X_valid.values), columns = final_features)\n",
    "\n",
    "print('imputed_X_train:', imputed_X_train.shape, ' imputed_X_valid:', imputed_X_valid.shape)\n",
    "print('Sample rows from imputed_X_train:')\n",
    "display_all(imputed_X_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Variable transformations on Input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numeric col names\n",
    "col_names = [f for f in imputed_X_train.columns if f in numeric_cols]\n",
    "\n",
    "# compute offset value for each column in the dataset\n",
    "offset_df = pd.DataFrame(imputed_X_train[col_names].min(), columns=['min_value'])\n",
    "offset_df['offset_value'] = offset_df['min_value'].apply(lambda x: 0 if (x>1 or x==1) else (1 if x==0 else (1-x if (x>0 and x<1) else abs(x)+1)))\n",
    "\n",
    "offset_df = offset_df['offset_value']\n",
    "print(offset_df.shape)\n",
    "display(offset_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaled dataset after adding offset values\n",
    "scaled_X_train = imputed_X_train[col_names].add(offset_df, axis='columns')\n",
    "scaled_X_valid = imputed_X_valid[col_names].add(offset_df, axis='columns')\n",
    "\n",
    "# replace all values<1 with 1 in validation data\n",
    "for x in col_names:\n",
    "    scaled_X_valid[x] = np.where(scaled_X_valid[x] < 1, 1, scaled_X_valid[x])\n",
    "\n",
    "print('scaled_X_train:', scaled_X_train.shape, ' scaled_X_valid:', scaled_X_valid.shape)\n",
    "print('Sample rows from scaled_X_train:')\n",
    "display_all(scaled_X_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable transformations\n",
    "# create the function transformer objects\n",
    "log_transform = FunctionTransformer(np.log, validate=True)\n",
    "sqrt_transform = FunctionTransformer(np.sqrt, validate=True)\n",
    "reciprocal_transform = FunctionTransformer(np.reciprocal, validate=True)\n",
    "exp_transform = FunctionTransformer(lambda x: x**(2), validate=True)\n",
    "yeo_johnson_transform = PowerTransformer(method='yeo-johnson', standardize=False)\n",
    "norm_transform = MinMaxScaler()\n",
    "std_transform = StandardScaler()\n",
    "\n",
    "# apply the transformation to the training data\n",
    "X_train_log = pd.DataFrame(log_transform.transform(scaled_X_train[col_names]), columns=col_names)\n",
    "X_train_sqrt = pd.DataFrame(sqrt_transform.transform(scaled_X_train[col_names]), columns=col_names)\n",
    "X_train_reciprocal = pd.DataFrame(reciprocal_transform.transform(scaled_X_train[col_names]), columns=col_names)\n",
    "X_train_exp = pd.DataFrame(exp_transform.transform(scaled_X_train[col_names]), columns=col_names)\n",
    "X_train_yeo_johnson = pd.DataFrame(yeo_johnson_transform.fit_transform(scaled_X_train[col_names]), columns=col_names)\n",
    "X_train_norm = pd.DataFrame(norm_transform.fit_transform(scaled_X_train[col_names]), columns=col_names)\n",
    "X_train_std = pd.DataFrame(std_transform.fit_transform(scaled_X_train[col_names]), columns=col_names)\n",
    "\n",
    "X_train_log = X_train_log.add_suffix('_log')\n",
    "X_train_sqrt = X_train_sqrt.add_suffix('_sqrt')\n",
    "X_train_reciprocal = X_train_reciprocal.add_suffix('_reciprocal')\n",
    "X_train_exp = X_train_exp.add_suffix('_exp')\n",
    "X_train_yeo_johnson = X_train_yeo_johnson.add_suffix('_yeo_johnson')\n",
    "X_train_norm = X_train_norm.add_suffix('_norm')\n",
    "X_train_std = X_train_std.add_suffix('_std')\n",
    "\n",
    "# concatenate all transformed dfs into one\n",
    "X_train_imp_transform = pd.concat([imputed_X_train, X_train_log, X_train_sqrt, X_train_reciprocal, X_train_exp, X_train_yeo_johnson, X_train_norm, X_train_std], axis=1)\n",
    "\n",
    "\n",
    "# apply the transformation to the validation data\n",
    "X_valid_log = pd.DataFrame(log_transform.transform(scaled_X_valid[col_names]), columns=col_names)\n",
    "X_valid_sqrt = pd.DataFrame(sqrt_transform.transform(scaled_X_valid[col_names]), columns=col_names)\n",
    "X_valid_reciprocal = pd.DataFrame(reciprocal_transform.transform(scaled_X_valid[col_names]), columns=col_names)\n",
    "X_valid_exp = pd.DataFrame(exp_transform.transform(scaled_X_valid[col_names]), columns=col_names)\n",
    "X_valid_yeo_johnson = pd.DataFrame(yeo_johnson_transform.transform(scaled_X_valid[col_names]), columns=col_names)\n",
    "X_valid_norm = pd.DataFrame(norm_transform.transform(scaled_X_valid[col_names]), columns=col_names)\n",
    "X_valid_std = pd.DataFrame(std_transform.transform(scaled_X_valid[col_names]), columns=col_names)\n",
    "\n",
    "X_valid_log = X_valid_log.add_suffix('_log')\n",
    "X_valid_sqrt = X_valid_sqrt.add_suffix('_sqrt')\n",
    "X_valid_reciprocal = X_valid_reciprocal.add_suffix('_reciprocal')\n",
    "X_valid_exp = X_valid_exp.add_suffix('_exp')\n",
    "X_valid_yeo_johnson = X_valid_yeo_johnson.add_suffix('_yeo_johnson')\n",
    "X_valid_norm = X_valid_norm.add_suffix('_norm')\n",
    "X_valid_std = X_valid_std.add_suffix('_std')\n",
    "\n",
    "# concatenate all transformed dfs into one\n",
    "X_valid_imp_transform = pd.concat([imputed_X_valid, X_valid_log, X_valid_sqrt, X_valid_reciprocal, X_valid_exp, X_valid_yeo_johnson, X_valid_norm, X_valid_std], axis=1)\n",
    "\n",
    "print('X_train_imp_transform:', X_train_imp_transform.shape, ' X_valid_imp_transform:', X_valid_imp_transform.shape)\n",
    "print('Sample rows from X_train_imp_transform:')\n",
    "display_all(X_train_imp_transform.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# binning numeric variables using KBinsDiscretizer\n",
    "col_names = [f for f in X_train_imp_transform.columns if f in numeric_cols]\n",
    "\n",
    "print(\"number of numeric cols in input data: \", len(col_names))\n",
    "\n",
    "# create the function transformer objects\n",
    "kmean = KBinsDiscretizer(n_bins=5, encode='onehot', strategy='kmeans')\n",
    "\n",
    "# apply the transformation to your data - training data\n",
    "X_train_kmean = pd.DataFrame.sparse.from_spmatrix(kmean.fit_transform(X_train_imp_transform[col_names]))\n",
    "\n",
    "# apply the transformation to your data - validation data\n",
    "X_valid_kmean = pd.DataFrame.sparse.from_spmatrix(kmean.transform(X_valid_imp_transform[col_names]))\n",
    "\n",
    "print('X_train_kmean:', X_train_kmean.shape, ' X_valid_kmean:', X_valid_kmean.shape)\n",
    "print(\"Info of X_train_kmean: \", X_train_kmean.info())\n",
    "print('Sample rows from X_train_kmean:')\n",
    "display_all(X_train_kmean.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating column names for the one-hot encoding columns from KBinsDiscretizer\n",
    "kmean_cols = []\n",
    "for n in range(len(col_names)):\n",
    "    for x in range(len(kmean.bin_edges_[n])-1):\n",
    "        l_val = str(round(kmean.bin_edges_[n][x],4))\n",
    "        h_val = str(round(kmean.bin_edges_[n][x+1],4))\n",
    "        col_nm = str(col_names[n])\n",
    "        tmp = col_nm+\"_\"+l_val+\"_to_\"+h_val\n",
    "        kmean_cols.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename column names of kmean transformed columns\n",
    "X_train_kmean.columns = X_valid_kmean.columns = kmean_cols\n",
    "print('number of one-hot columns created:', len(kmean_cols))\n",
    "\n",
    "# concating discretized numeric cols with former transformed dataset\n",
    "X_train_imp_transform.reset_index(drop=True, inplace=True)\n",
    "X_valid_imp_transform.reset_index(drop=True, inplace=True)\n",
    "X_train_kmean.reset_index(drop=True, inplace=True)\n",
    "X_valid_kmean.reset_index(drop=True, inplace=True)\n",
    "\n",
    "X_train_imp_transform = pd.concat([X_train_imp_transform, X_train_kmean], axis=1)\n",
    "X_valid_imp_transform = pd.concat([X_valid_imp_transform, X_valid_kmean], axis=1)\n",
    "\n",
    "print('X_train_imp_transform:', X_train_imp_transform.shape, ' X_valid_imp_transform:', X_valid_imp_transform.shape)\n",
    "print('sample rows from X_train_imp_transform:')\n",
    "display_all(X_train_imp_transform.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # convert data types to store effectively\n",
    "X_train_imp_transform = X_train_imp_transform.astype(np.float32)\n",
    "X_valid_imp_transform = X_valid_imp_transform.astype(np.float32)\n",
    "\n",
    "# # replace NaNs and infs with zeroes(if present)\n",
    "X_train = pd.DataFrame(np.nan_to_num(X_train_imp_transform, posinf=0, neginf=0), columns=X_train_imp_transform.columns)\n",
    "X_valid = pd.DataFrame(np.nan_to_num(X_valid_imp_transform, posinf=0, neginf=0), columns=X_valid_imp_transform.columns)\n",
    "\n",
    "# print info about the dataset\n",
    "print('Info of X_train: ')\n",
    "print(X_train.info())\n",
    "print('Presence of NaNs: ', np.any(np.isnan(X_train)))\n",
    "print('Presence of Infs: ', np.all(np.isinf(X_train)))\n",
    "print('Only Finite values: ', np.all(np.isfinite(X_train)))\n",
    "print('-'*60)\n",
    "print('Info of X_valid: ')\n",
    "print(X_valid.info())\n",
    "print('Presence of NaNs: ', np.any(np.isnan(X_valid)))\n",
    "print('Presence of Infs: ', np.all(np.isinf(X_valid)))\n",
    "print('Only Finite values: ', np.all(np.isfinite(X_valid)))\n",
    "\n",
    "display_all(X_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing features with low variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing features with low variance (threshold=0.95)\n",
    "threshold_n=0.95\n",
    "sel = VarianceThreshold(threshold=(threshold_n* (1 - threshold_n) ))\n",
    "sel.fit_transform(X_train)\n",
    "sel_var_features = X_train.columns[sel.get_support(indices=True)]\n",
    "print('Total number of features after removing low variance features:', len(sel_var_features))\n",
    "      \n",
    "# retaining only selected columns \n",
    "X_train = X_train[sel_var_features]\n",
    "X_valid = X_valid[sel_var_features]\n",
    "\n",
    "fs_lowvar = sel_var_features\n",
    "\n",
    "pd.DataFrame(sel_var_features).to_csv(f\"{fs_path}\\\\03_FeaturesList_after_removing_Low_Variance.csv\", index=False, header=['column_names'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting features based on Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating correlation matrix\n",
    "corr_mat = X_train.corr().abs()\n",
    "corr_mat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# select upper triangle of correlation matrix\n",
    "upper = corr_mat.where(np.triu(np.ones(corr_mat.shape), k=1).astype(np.bool))\n",
    "# upper.to_csv(f'{PATH}corr_matrix.csv', index=False)\n",
    "\n",
    "# Find index of feature columns with correlation greater than 0.8\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.8)]\n",
    "\n",
    "# Drop features \n",
    "X_train.drop(to_drop, axis=1, inplace=True)\n",
    "X_valid.drop(to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(\"total number of features after removing highly correlated features: \", len(X_train.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating correlation matrix\n",
    "new_corr = X_train.corr().abs()\n",
    "\n",
    "# generating the correlation heatmap\n",
    "plt.subplots(figsize=(20, 12))\n",
    "sns.heatmap(new_corr, vmax=.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection after applying Variable transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection with Boruta (after variable transformations)\n",
    "clf = RandomForestClassifier(n_estimators=50, n_jobs=-1, max_depth=5, random_state=0)\n",
    "\n",
    "boruta_feature_selector = BorutaPy(clf, n_estimators=100, random_state=42, verbose=2, max_iter=50, perc=50)\n",
    "boruta_feature_selector.fit(X_train.values, y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating list of selected features (using boruta)\n",
    "features = X_train.columns\n",
    "\n",
    "boruta_features = list()\n",
    "indexes = np.where(boruta_feature_selector.support_ == True)\n",
    "for x in np.nditer(indexes):\n",
    "    boruta_features.append(features[x])\n",
    "print('Number of features selected by Boruta: ', len(boruta_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection with Univariate Statistical Tests - ANOVA F value\n",
    "# please check the 'k' value - number of features to be selected, this should be less than total # of features in train data\n",
    "\n",
    "# summarize scores\n",
    "m1_selector = SelectKBest(score_func=f_classif, k=100)\n",
    "m1_selector.fit(X_train, y_train)\n",
    "\n",
    "# Summarize scores\n",
    "np.set_printoptions(precision=3)\n",
    "# display_all(m1_selector.scores_)\n",
    "\n",
    "# create list with selected features\n",
    "m1_cols = m1_selector.get_support(indices=True)\n",
    "m1_features = list(X_train.iloc[:,m1_cols].columns)\n",
    "print('Number of features selected by ANOVA:', len(m1_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print features and their ANOVA score\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "anova_df = pd.DataFrame(m1_selector.scores_, columns = [\"ANOVA\"], index=X_train.columns).reset_index()\n",
    "anova_df.sort_values('ANOVA', ascending=0, inplace=True)\n",
    "anova_df.reset_index(drop=True, inplace=True)\n",
    "anova_df = anova_df[anova_df['index'].isin(m1_features)]  \n",
    "\n",
    "display_all(anova_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection with RFE(recursive feature elimination)\n",
    "# please check the n_features_to_select, this should be less than total # of features in train data\n",
    "\n",
    "model = LogisticRegression()\n",
    "rfe = RFE(model, n_features_to_select=100)\n",
    "fit = rfe.fit(X_train, y_train)\n",
    "print(\"Number of features selected by RFE: %s\" % (fit.n_features_))\n",
    "\n",
    "# create list with selected features\n",
    "feat_names = X_train.columns;\n",
    "m2_selector=fit.get_support()\n",
    "m2_features = list(feat_names[m2_selector])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe_df = pd.DataFrame(fit.support_, columns = [\"RFE\"], index=X_train.columns).reset_index()\n",
    "rfe_df = (rfe_df[rfe_df['RFE'] == True]).reset_index(drop=True)\n",
    "\n",
    "display_all(rfe_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection with Extra Trees Classifier\n",
    "model = ExtraTreesClassifier(n_estimators=50)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing feature importance metric\n",
    "feature_imp_normalized = np.std([tree.feature_importances_ for tree in model.estimators_], axis = 0) \n",
    "\n",
    "# list of selected features (using feature importance from Extratrees classifier)\n",
    "idx = np.arange(0, X_train.shape[1]) #create an index array, with the number of features\n",
    "\n",
    "m3_selector = idx[feature_imp_normalized > np.mean(feature_imp_normalized)]\n",
    "print(\"Number of features selected by Extratrees Classifier: \", len(m3_selector))\n",
    "feature_names = X_train.columns;\n",
    "m3_features = list(feature_names[m3_selector])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dataframe with selected features from extratrees classifier\n",
    "vi_df = pd.DataFrame(model.feature_importances_, columns = [\"Extratrees\"], index=X_train.columns).reset_index()\n",
    "vi_df.sort_values(['Extratrees'], ascending=0, inplace=True)\n",
    "vi_df.reset_index(drop=True, inplace=True)\n",
    "vi_df = vi_df[vi_df['index'].isin(m3_features)]  \n",
    "vi_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "display_all(vi_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final list of features, which are common across atleast three feature selection methods\n",
    "min_threshold = 3\n",
    "combined_features = (boruta_features) + (m1_features) + (m2_features) + (m3_features)\n",
    "feat_cnt = Counter(combined_features)\n",
    "\n",
    "for key, cnts in list(feat_cnt.items()):   \n",
    "    if cnts < min_threshold:\n",
    "        del feat_cnt[key]\n",
    "        \n",
    "final_feature_list = list(feat_cnt.keys())\n",
    "print(\"Number of features in final list:\", len(final_feature_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the feature selection results into a excel\n",
    "with pd.ExcelWriter(f'{fs_path}\\\\04_Feature_Selection_Results.xlsx') as writer:\n",
    "    pd.DataFrame(boruta_features, columns=['Selected Features']).to_excel(writer, sheet_name='Boruta')\n",
    "    pd.DataFrame(anova_df).to_excel(writer, sheet_name='Univariate_Anova')\n",
    "    pd.DataFrame(rfe_df[rfe_df['RFE'] == True]).to_excel(writer, sheet_name='RFE')\n",
    "    pd.DataFrame(vi_df).to_excel(writer, sheet_name='ExtratreesClassifier Importance')\n",
    "    pd.DataFrame(final_feature_list, columns=['Selected Features']).to_excel(writer, sheet_name='Final Features List')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated training & validation datasets with final features only\n",
    "X_train = X_train[final_feature_list]\n",
    "X_valid = X_valid[final_feature_list]\n",
    "\n",
    "print('X_train:', X_train.shape, ' X_valid:', X_valid.shape, ' y_train:', y_train.shape, ' y_valid:', y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying multicollinearity to remove columns which are dependent on each other, Threshold=2.5\n",
    "\n",
    "class ReduceVIF(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, thresh=2.5, impute=True, impute_strategy='median'):\n",
    "        self.thresh = thresh\n",
    "        \n",
    "        # The statsmodel function will fail with NaN values, as such we have to impute them.\n",
    "        # By default we impute using the median value.\n",
    "        # This imputation could be taken out and added as part of an sklearn Pipeline.\n",
    "        if impute:\n",
    "            self.imputer = SimpleImputer(missing_values=np.nan, strategy=impute_strategy)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        print('ReduceVIF fit')\n",
    "        if hasattr(self, 'imputer'):\n",
    "            self.imputer.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        print('ReduceVIF transform')\n",
    "        columns = X.columns.tolist()\n",
    "        if hasattr(self, 'imputer'):\n",
    "            X = pd.DataFrame(self.imputer.transform(X), columns=columns)\n",
    "        return ReduceVIF.calculate_vif(X, self.thresh)\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_vif(X, thresh=2.5):\n",
    "        dropped=True\n",
    "        while dropped:\n",
    "            variables = X.columns\n",
    "            dropped = False\n",
    "            vif = [variance_inflation_factor(X[variables].values, X.columns.get_loc(var)) for var in X.columns]\n",
    "            \n",
    "            max_vif = max(vif)\n",
    "            if max_vif > thresh:\n",
    "                maxloc = vif.index(max_vif)\n",
    "                print(f'Dropping {X.columns[maxloc]} with vif={np.round(max_vif,2)}')\n",
    "                X = X.drop([X.columns.tolist()[maxloc]], axis=1)\n",
    "                dropped=True\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns having higher VIF factor ot having high multicollinearity\n",
    "vif = ReduceVIF()\n",
    "X_train = vif.fit_transform(X_train)\n",
    "X_valid = X_valid[list(X_train.columns)]\n",
    "\n",
    "print('X_train:', X_train.shape, ' X_valid:', X_valid.shape, ' y_train:', y_train.shape, ' y_valid:', y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing VIF for the final list of features\n",
    "def calculate_vif(X):\n",
    "    vif = pd.DataFrame()\n",
    "    vif[\"Features\"] = X.columns\n",
    "    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "    return(vif)\n",
    "\n",
    "vif = calculate_vif(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the final of features wth their VIF & feature importances\n",
    "model = ExtraTreesClassifier(n_estimators=50)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "vi_df = pd.DataFrame(model.feature_importances_, columns = [\"Importances\"], index=X_train.columns).reset_index()\n",
    "vi_df.sort_values(['Importances'], ascending=0, inplace=True)\n",
    "vi_df.columns = ['Features', 'Importances']\n",
    "\n",
    "vif = vif.merge(vi_df, on='Features')\n",
    "vif.to_csv(f\"{fs_path}\\\\05_Feature_Importances_with_VIF.csv\", index=False)\n",
    "display_all(vif)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
